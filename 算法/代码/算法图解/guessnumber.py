# encoding: utf-8
import os
import urllib.request
import urllib.error
import urllib.parse
import json
import re
import time


class MMSpider:

	def __init__(self):
		self.__code_type = "gbk"
		self.__http = "http:"
		self.__url = "http://mm.taobao.com/tstar/search/tstar_model.do?_input_charset=utf-8"
		# 每个模特主页的地址
		self.__base_person_url = "http://mm.taobao.com/self/aiShow.htm?userId="
		# 相册的地址
		self.__base_all_album_url = "https://mm.taobao.com/self/album/open_album_list.htm?_charset=utf-8&user_id%20="
		# 具体相册地址
		self.__base_pic_url = "https://mm.taobao.com/album/json/get_album_photo_list.htm?user_id="

		# r表示python的原生字符串 每个模特创建一个文件夹，表示存储的地址
		self.__base_path = r"F:\TaoBaoMM"
		# 想要提取的页数
		self.__total_page = 1
		# 当前正在提取的页数
		self.__currentPage = 1
		# 找到具体album的id的正则表达式
		self.__album_pattern = re.compile('''<h4>.*?album_id=(.*?)&''', re.S)
		# album有多少的的正则表达式
		self.__album_page_pattern = re.compile('''<input name="totalPage" id="J_Totalpage" value="(.*?)"''', re.S)

	# 由页面返回的json数据得到字典,会得到30个模特的具体信息
	def get_person_dict(self, currentPage):
		try:
			data = urllib.parse.urlencode({"currentPage": currentPage}).encode("utf-8")
			request = urllib.request.Request(self.__url, data=data)
			response = urllib.request.urlopen(request)
			result = response.read().decode(self.__code_type)
			return json.loads(result)
		except urllib.error.URLError as e:
			print("出错了！", e.reason)

	# 保存信息MM的文字图片信息
	def save(self, searchDOList):
		# 进入每个模特的主页，找到相册地址
		# 保存对每个模特创建一个文件夹，保存文字信息并进入相册下载图片
		for person in searchDOList:
			# 创建文件夹，保持信息
			dir_path = self.__base_path + "\\" + person["realName"]
			if self.mkdir(dir_path):
				# 模特个人主页的url
				txt_path = dir_path + "\\" + person["realName"] + ".txt"
				self.write_txt(txt_path, person)
				self.save_imgs(person, dir_path)

	# 每个MM保存1000张照片 每个人照片实在太多了
	def save_imgs(self, person, dir_path):
		# 找到MM的相册一共有多少页
		album_page = self.get_album_page(person["userId"])
		img_index = 1
		# 在每一页中操作找到该页中album的id
		for i in range(1, int(album_page) + 1):
			album_ids = self.get_album_ids(person["userId"], i)
			for album_id in album_ids:
				pic_page = self.get_pic_page(person["userId"], album_id)
				# 将每一个中的图片的地址保存下来
				for j in range(1, int(pic_page) + 1):
					imgs_url = self.get_imgs_url(person, j, album_id)
					for img_url in imgs_url:
						try:
							url = self.__http + img_url["picUrl"]
							response = urllib.request.urlopen(url, timeout=5)
							with open(dir_path + "\\" + str(img_index) + ".jpg", "wb") as file:
								file.write(response.read())
								if img_index % 100 == 0:
									print("sleep 1 second")
									time.sleep(1)
								if img_index >= 1000:
									print(person["realName"] + ":已经保存1000张辣")
									return
								img_index = img_index + 1
						except TimeoutError as e:
							print(e.strerror)
						except urllib.error.URLError as e:
							print(e.reason)
						except BaseException as e:
							print(e.args)

	# 创建新的文件夹
	def mkdir(self, dir_path):
		if os.path.exists(dir_path):
			return False
		# 文件夹不存在时才创建
		else:
			os.mkdir(dir_path)
			return True

	# 新建txt文件并将文字信息写入txt文件
	def write_txt(self, txt_path, person):
		person_url = self.__base_person_url + str(person["userId"])
		content = "姓名：" + person["realName"] + "  城市：" + person["city"] \
				  + "\n身高：" + str(person["height"]) + "  体重：" + str(person["weight"]) \
				  + "\n喜欢：" + str(person["totalFavorNum"]) \
				  + "\n个人主页：" + person_url
		with open(txt_path, "w", encoding="utf-8") as file:
			print(person["realName"] + ":正在保存" + person["realName"] + "的文字信息")
			file.write(content)

	# 保存图片的地址
	def get_imgs_url(self, person, j, album_id):
		url = self.__base_pic_url + str(person["userId"]) \
			  + "&album_id=" + str(album_id) \
			  + "&page=" + str(j)
		try:
			response = urllib.request.urlopen(url, timeout=5)
			result = response.read().decode(self.__code_type)
			imgs_url = json.loads(result)["picList"]
			return imgs_url
		except TimeoutError as e:
			print(e.strerror)
		except urllib.error.URLError as e:
			print(e.reason)
		except BaseException as e:
			print(e.args)

	# 由第page页找到该页中所有album的id
	def get_album_ids(self, userId, page):
		try:
			all_album_url = self.__base_all_album_url + str(userId) + "&" + str(page)
			request = urllib.request.Request(all_album_url)
			response = urllib.request.urlopen(request)
			html = response.read().decode(self.__code_type)
			# 提取该页中album的id
			return re.findall(self.__album_pattern, html)
		except urllib.error.URLError as e:
			print("提取相册id出错了！", e.reason)

	# 找到一个相册内的图片有多少页
	def get_pic_page(self, userId, albumId):
		try:
			# 先得到这个相册一共有多少页
			url = self.__base_pic_url + str(userId) + "&album_id=" + str(albumId)
			response = urllib.request.urlopen(url)
			result = json.loads(response.read().decode(self.__code_type))
			return result["totalPage"]
		except urllib.error.URLError as e:
			print(e.reason)
			return None

	# 得到MM一共有多少页的相册
	def get_album_page(self, userId):
		try:
			all_album_url = self.__base_all_album_url + str(userId)
			response = urllib.request.urlopen(all_album_url)
			html = response.read().decode(self.__code_type)
			return re.search(self.__album_page_pattern, html).group(1)
		except urllib.error.URLError as e:
			print("得到MM一共有多少页的相册出错啦", e.reason)
		return None

	def start(self):
		print("开始辣！")
		opener = urllib.request.build_opener()
		opener.addheaders = [("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.96 Safari/537.36")]
		urllib.request.install_opener(opener)
		for i in range(self.__total_page):
			dict_result = self.get_person_dict(self.__currentPage)
			searchDOList = dict_result["data"]["searchDOList"]
			# 保存所有本页中MM的信息
			self.save(searchDOList)
			self.__currentPage += 1

if __name__ == "__main__":
	spider = MMSpider()
	spider.start()